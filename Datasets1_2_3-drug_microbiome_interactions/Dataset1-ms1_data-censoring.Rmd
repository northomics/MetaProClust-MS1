---
title: "MS1 clustering with data censoring"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

MS1 data is inherently noisy. MS1 captures is not filtered (like DDA MS/MS). In addition, we aren't matching spectra to peptides and thus are capturing **everything** in the sample, not just proteins.

What if we censored our MS1 data and only clustered the highest intensity values? 

First we load the intensity data from our MS1. We identify quartiles in our data and consider different minimum values as missing in our data filtering step.
```{r}
base::load(here::here("data", "ms1int.RData"))
library(impute)
library(pvclust)
library(tidyverse)
library(DESeq2)
library(dendextend)
library(cluster)
### functions
is.nan.data.frame <- function(x){
    do.call(cbind, lapply(x, is.nan))}


filter_valids = function(df, conditions, min_count, at_least_one = TRUE) {
    # df = data frame containing LOG2 data for filtering and organized by data type
    # conditions = a character vector dictating the grouping
    # min_count = a numeric vector of the same length as "conditions" indicating the minimum 
    #     number of valid values for each condition for retention
    # at_least_one = TRUE means to keep the row if min_count is met for at least one condition
    #     FALSE means min_count must be met across all conditions for retention
    #df[df==0] <- NA
    df[is.nan.data.frame(df)] <- 0
    all_names <- colnames(df) 
    cond.names = lapply(conditions, # Group column names by conditions
                        function(x) grep(x, all_names, value = TRUE, perl = TRUE))
    cond.filter = sapply(1:length(cond.names), function(i) {
        df2 = df %>% dplyr::select(cond.names[[i]])   # Extract columns of interest
        df2 = as.matrix(df2)   # Cast as matrix for the following command
        sums = rowSums(df2!=0) # count the number of valid values for each condition
        sums >= min_count[i]   # Calculates whether min_count requirement is met
    })
    if (at_least_one) {
        df$KEEP = apply(cond.filter, 1, any)
    } else {
        df$KEEP = apply(cond.filter, 1, all)
    }
    #return(df) # No rows are omitted, filter rules are listed in the KEEP column
    
    return(df)  # only keeping rows that meet the criteria!
}

filter_valids_quart = function(df, conditions, min_count, int_remove, at_least_one = TRUE) {
    # df = data frame containing LOG2 data for filtering and organized by data type
    # conditions = a character vector dictating the grouping
    # min_count = a numeric vector of the same length as "conditions" indicating the minimum 
    #     number of valid values for each condition for retention
    # at_least_one = TRUE means to keep the row if min_count is met for at least one condition
    #     FALSE means min_count must be met across all conditions for retention
    #df[df==0] <- NA
    df[is.nan.data.frame(df)] <- 0
    df[df <= int_remove] <- 0
    all_names <- colnames(df) 
    cond.names = lapply(conditions, # Group column names by conditions
                        function(x) grep(x, all_names, value = TRUE, perl = TRUE))
    cond.filter = sapply(1:length(cond.names), function(i) {
        df2 = df %>% dplyr::select(cond.names[[i]])   # Extract columns of interest
        df2 = as.matrix(df2)   # Cast as matrix for the following command
        sums = rowSums(df2!=0) # count the number of valid values for each condition
        sums >= min_count[i]   # Calculates whether min_count requirement is met
    })
    if (at_least_one) {
        df$KEEP = apply(cond.filter, 1, any)
    } else {
        df$KEEP = apply(cond.filter, 1, all)
    }
    #return(df) # No rows are omitted, filter rules are listed in the KEEP column
    
    return(df)  # only keeping rows that meet the criteria!
}



## Cluster using top intensity qualtiles, top 2 quartiles, top 3 quatiles...etc
table(int_4_quartiles)
int$quart <- int_4_quartiles
max_to_remove <- int %>% filter(quart == "remove")
max_to_remove <- max_to_remove$intensity %>% max()

high_min <- int %>% filter(quart == "High") %>% 
    dplyr::select(intensity) %>% min()
med_min <- int %>% filter(quart == "Med") %>% 
    dplyr::select(intensity) %>% min()
low_min <- int %>% filter(quart == "Low") %>% 
    dplyr::select(intensity) %>% min()

print(high_min)
print(med_min)
## change everything that's below the minimum value of the quatile -> NA
## Then filter the same way as before
high_quart_int <- intensity
high_quart_int[high_quart_int < high_min] <- 0
    
med_quart_int <- intensity
med_quart_int[med_quart_int < med_min] <- 0

```

## Data filtering according the intensity quartiles.

```{r}
## Filter using this censored data
high_filt <- filter_valids(high_quart_int, 
                               conditions = cond_opts,
                               min_count = cond_count, 
                               at_least_one = T)  
high_filt <- high_quart_int[high_filt$KEEP == 1,]  ## 12,014

med_filt <- filter_valids(med_quart_int, 
                           conditions = cond_opts,
                           min_count = cond_count, 
                           at_least_one = T)  
med_filt <- med_quart_int[med_filt$KEEP == 1,]  ## 24,851


#### normalize by depth
high_norm <- estimateSizeFactorsForMatrix(high_filt)
high_norm <- sweep(high_filt, 2, high_norm, "/") ##peptides are normalized

med_norm <- estimateSizeFactorsForMatrix(med_filt)
med_norm <- sweep(med_filt, 2, med_norm, "/") ##peptides are normalized

```

## Missing data imputation

```{r}
## impute missing values
high_norm[high_norm== 0] <- NA
med_norm[med_norm== 0] <- NA


high_missing <- data.frame(num_missing = colSums(is.na(high_norm)),
                           total = rep(nrow(high_norm), ncol(high_filt))) %>%
    mutate(perc_missing = (num_missing/total) * 100)

high_imputed<- impute.knn(high_norm %>% as.matrix(), k = 10, rowmax = 0.5, 
                          colmax = 0.96, rng.seed = 362436069)$data
high_imputed[high_imputed==0] <- 1
med_imputed<- impute.knn(med_norm %>% as.matrix(), k = 10, rowmax = 0.5, 
                          colmax = 0.96, rng.seed = 362436069)$data

colnames(high_imputed) <- colnames(intensity)
colnames(med_imputed) <- colnames(intensity)
```

## Log2 transformation and fold change calculation

```{r}
# Transform by log2
high_log <- data.frame(high_imputed) %>% 
    mutate_all(., funs(log2(.)))
colnames(high_log) = colnames(high_imputed)

med_log <- data.frame(med_imputed) %>% 
    mutate_all(., funs(log2(.)))
colnames(med_log) = colnames(med_imputed)


## log fc
#high_ctrl <- dplyr::select(high_log, contains("NC"))
#high_ctrl$median = apply(high_ctrl, 1, median, na.rm = T)
#high_drug_temp <- dplyr::select(high_log, -contains("NC"))
#high_ctrl<- matrix(high_ctrl$median, nrow=length(high_ctrl$median), 
#                   ncol=ncol(high_drug_temp), byrow=F)
#high_fc <- high_ctrl - high_drug_temp  %>% dplyr::select(noctrlmeta$Name)
##high_fc$feat_num <- rownames(high_filt)
#
#med_ctrl <- dplyr::select(med_log, contains("NC"))
#med_ctrl$median = apply(med_ctrl, 1, median, na.rm = T)
#med_drug_temp <- dplyr::select(med_log, -contains("NC"))
#med_ctrl<- matrix(med_ctrl$median, nrow=length(med_ctrl$median), 
#                   ncol=ncol(med_drug_temp), byrow=F)
#med_fc <- med_ctrl - med_drug_temp %>% dplyr::select(noctrlmeta$Name)
#med_fc$feat_num <- rownames(med_filt)

#low_fc$feat_num <- rownames(low_filt)

write.csv(high_log, here::here("data", "Dataset1-ms1_high", "high_log2.txt"), 
          quote=F, row.names = T)
write.csv(med_log, here::here("data", "Dataset1-ms1_med", "med_log2.txt"), 
          quote=F, row.names = T)
```

## PCA of censored data

```{r}
metadata <- read.csv(here::here("data", "meta_data.csv")) %>%
    mutate(plate_cell = paste0("P2", Cell))
#int_col <- colnames(intensity) %>% data.frame(.)
#metadata <- left_join(int_col, metadata, by=c("." = "plate_cell")) ## only including
#meta_drug <- metadata %>% arrange(., Drug)

###
high_pca <- prcomp(high_log %>% t(), center=F, scale=F)
high_sampleVals<-data.frame(high_pca$x)
high_exprVals<-data.frame(high_pca$rotation)
high_PoV <- (high_pca$sdev^2/sum(high_pca$sdev^2))*100 

#high_sampleVals <- high_sampleVals %>% select(metadata$Name)
high_coords <- merge(high_sampleVals, metadata, by.x=0, by.y="Name")

#high_coords<-data.frame(high_sampleVals, Drug = noctrlmeta$Drug,
#                   samplename = rownames(high_sampleVals),
#                   Concentration = noctrlmeta$Conc)
high_coords$Conc<- factor(high_coords$Conc, levels = c("H", "M", "L", "1D"))
high_numPCs <- 1:length(high_PoV)

for (i in 1:length(high_PoV)) {
    percent <- paste0("(", round(high_PoV[i],2), "%)")
    name <- paste0("PC", i, "per")
    assign(name, percent)
}

med_pca <- prcomp(med_log %>% t(), center=F, scale=F)
med_sampleVals<-data.frame(med_pca$x)
med_exprVals<-data.frame(med_pca$rotation)
med_PoV <- (med_pca$sdev^2/sum(med_pca$sdev^2))*100 

med_coords <- merge(med_sampleVals, metadata, by.x=0, by.y="Name")
med_coords$Conc <- factor(med_coords$Conc, levels = c("H", "M", "L", "1D"))
med_numPCs <- 1:length(med_PoV)

for (i in 1:length(med_PoV)) {
    percent <- paste0("(", round(med_PoV[i],2), "%)")
    name <- paste0("PC", i, "per")
    assign(name, percent)
}


quick_pca <- function(coords_df, x_PC, y_PC, PoV, shape, fill){
    yaxis <- y_PC
    xaxis <- x_PC
    yperc <- paste0("(", round(PoV[yaxis %>% as.numeric()] ,2), "%)")
    xperc <- paste0("(", round(PoV[xaxis %>% as.numeric()] ,2), "%)")
    
    yaxislabel <- paste0("PC", yaxis, " ", yperc)
    xaxislabel <- paste0("PC", xaxis, " ", xperc)
    pca_plot <- ggplot(coords_df, aes_string(x = paste0('PC', xaxis), y = paste0('PC', yaxis))) + #accept selectInput to choose axes!
        geom_point(size=4, aes_string(shape = shape, color = fill)) + 
        #scale_color_manual(values=c("#3444DA",  "#FFBA49", "#8C5999", "#D7C0D0", "#363635", "#DF737A", "#20A39E", "#8EB8E5")) + 
       # scale_color_manual(values=c("#3444DA",  "#DF737A", "#8C5999", "#D7C0D0", "#FFBA49")) +
        scale_x_continuous(name=xaxislabel) + # labels depend on selected PCs
        scale_y_continuous(name=yaxislabel) +
        theme_bw() +
        theme(legend.position = "right", text=element_text(size=16))
    return(pca_plot)
}
(pc12high <- quick_pca(coords_df = high_coords, x_PC = 1, y_PC = 2, PoV=high_PoV, shape = "Conc", fill = "Drug"))
#(pc23 <- quick_pca(coords_df = high_coords, x_PC = 2, y_PC = 3, shape = "Concentration", fill = "Drug"))
(pc12med <- quick_pca(coords_df = med_coords, x_PC = 1, y_PC = 2,  PoV=med_PoV, shape = "Conc", fill = "Drug"))

```

## Clustering 

Clustering can be completed the following scripts provided in `bin/`. The ICA implementation of precise-db requires a conda environment which can be accessed using `conda activate precise-db`.

This was explained in previous notebooks.

## Eigenfeature calculation and clustering

```{r}
highkmedoid_labels <- read.csv(here::here("data", "Dataset1-ms1_high", "k-med_labels.csv"), header=F) %>% as.vector()

medkmedoid_labels <- read.csv(here::here("data","Dataset1-ms1_med", "k-med_labels.csv"), header=F) %>% as.vector()

## clustering function
autoCluster <- function(fc_data, kmed_labels, fc_metadata, n_boot=100){
    clusterinfo <- kmed_labels %>% data.frame()
    highmax_clust <- kmed_labels %>% unique() %>% max()
    fc_clusters <- list()
    for (x in 0:highmax_clust){
        clustername <- paste("Cluster", x)
        y <- kmed_labels[which(clusterinfo$V1 == x),,drop=F] %>% rownames()
        fc_clusters[[clustername]] <- fc_data[y %>% as.numeric(),]
        fc_clusters[[clustername]]$feat_num <- y
    }
    #pca_clusters <- lapply(fc_clusters, function(x) prcomp(x %>% dplyr::select(-feat_num) %>% t(), center=T, scale=F)$x)
    #pc1_cluster <- lapply(pca_clusters,  "[", , 1)
    svd_clusters <- lapply(fc_clusters, function(x) svd(x %>% dplyr::select(-feat_num))$v)
# grabbing the first principal component
    svd1_cluster <- lapply(svd_clusters,  "[", , 1)
    clustereigengenes <- bind_cols(svd1_cluster) %>% data.frame()
    ### make a factored dataframe of the compounds
    fc_metadata <- fc_metadata %>%
      mutate(Drug = case_when(Drug == "NC" ~ "DMSO",
                          TRUE ~ Drug),
         Concentration = case_when(Conc == "1D" ~ "NA",
                                   TRUE ~ Conc),
         drug_conc = paste0(Drug, "-", Conc),
         drug_conc = case_when(drug_conc == 'DMSO-1D' ~ 'DMSO',
                                                        TRUE ~ drug_conc)
     )
      
    datTraits <- data.frame(compounds= as.factor(fc_metadata$drug_conc))
    rownames(datTraits) <- fc_metadata$Name
    traits <- model.matrix(~ ., data=datTraits,
                           contrasts.arg = lapply(datTraits, contrasts, contrasts=FALSE))
    traits <- traits[,-1] ##removing intercept
    moduleTraitCor <- stats::cor(clustereigengenes, traits, method = "s")
    colnames(moduleTraitCor) <- colnames(moduleTraitCor) %>% substr(., 10, nchar(colnames(moduleTraitCor)))
    ###hclust clusters the rows
    module_clust <- hclust(as.dist(1-cor(t(moduleTraitCor), method="pearson")), method="average")
    treat_clust <- hclust(as.dist(1-cor(moduleTraitCor, method="pearson")), method="average")
    treat_clust_boot <- pvclust(moduleTraitCor, method.hclust="average",
                                method.dist="correlation", nboot=100,
                                iseed=2248)
    toreturn <- list("bootstrap" = treat_clust_boot, "cluster_order" = module_clust, 
                     "cluster_cor" = moduleTraitCor)
    return(toreturn)
}

high_clustering <- autoCluster(high_log, highkmedoid_labels, metadata)
med_clustering <- autoCluster(med_log, medkmedoid_labels, metadata)

high_clust_boot <- as.dendrogram(high_clustering$bootstrap)
high_clustering$bootstrap %>% plot(main = "High intensity")
high_clustering$bootstrap %>% pvrect(alpha=0.9, pv="au")

med_clust_boot <- as.dendrogram(med_clustering$bootstrap)
med_clustering$bootstrap %>% plot(main = "Medium intensity")
med_clustering$bootstrap %>% pvrect(alpha=0.9, pv="au")


```

Calculating "optimal" number of clusters for each dataset.

```{r}
quick_sil <- function(hclust_out, distance, max_k){
  silhouette_score <- function(i){
    k=cutree(hclust_out,i)
    ss <- silhouette(k, 1-cor(distance, method="pearson"))
    mean(ss[, 3])
    }
  clustnum <- 2:max_k
  avg_sil <- sapply(clustnum, silhouette_score)
  avg_sil <- data.frame(num = clustnum, sil_score = avg_sil)
  avg_sil <- rbind(c(1,0), avg_sil)
  return(avg_sil)
}

## High intensities
high_sil <- quick_sil(hclust_out=high_clustering$bootstrap, distance = 1-cor(high_clustering$cluster_cor, method="pearson"), max_k = 7) 

(high_sil <- ggplot(high_sil, aes(x=num, y = sil_score)) +
  geom_point(size=4, colour="#F25C54", alpha=1) +
    geom_line(size=2, alpha=0.4) +
    geom_vline(xintercept=4, size = 2, linetype="dashed", color = "#38618C") +
    xlab("Number of clusters") +
    ylab("Average Silhouette Score") +
    scale_x_continuous(breaks = 1:30) +
    theme_classic(base_size=14) +
    ggtitle("High intensities")
)


## med intensities
med_sil <- quick_sil(hclust_out=med_clustering$bootstrap, distance = 1-cor(med_clustering$cluster_cor, method="pearson"), max_k = 7) 

(med_sil <- ggplot(med_sil, aes(x=num, y = sil_score)) +
  geom_point(size=4, colour="#F25C54", alpha=1) +
    geom_line(size=2, alpha=0.4) +
    geom_vline(xintercept=4, size = 2, linetype="dashed", color = "#38618C") +
    xlab("Number of clusters") +
    ylab("Average Silhouette Score") +
    scale_x_continuous(breaks = 1:30) +
    theme_classic(base_size=14) +
    ggtitle("High and med intensities")
)



```

## Comparing clustering results of med + high

```{r}
pink <- "#DF737A"
blackcof <- "#2E1F27"
teal <- "#59C9A5"
yellow <- "#FFE66D"
blue <- "#3444DA"


clust_cols_med <- c(pink, teal, blue,yellow)
clust_cols_high <- c(pink, teal, yellow,blue)

## ms1 optimal clust num = 2
high_svd <- high_clustering$bootstrap %>% as.dendrogram() %>% color_branches(k=4, col=clust_cols_high) %>%
    dendextend::set("branches_lwd", 3) 
## ms1 optimal clust num = 2
med_svd <- med_clustering$bootstrap %>% as.dendrogram() %>% color_branches(k=4, col=clust_cols_med) %>%
    dendextend::set("branches_lwd", 3) 
## ms1 optimal clust num = 2
set.seed(123)
compare_highmed <- dendlist(high_svd, med_svd) %>% untangle(method = "step2side")
compare_highmed %>% entanglement()

cor_cophenetic(high_svd,med_svd)

pdf("../../4.mSystems/2.revisionv2/figs/tanglegram-highmed.pdf", width=6, height=5)
tanglegram(compare_highmed, sort=F, common_subtrees_color_lines=F,highlight_branches_lwd = F, 
           highlight_distinct_edges=F, main_left="High", main_right="High+Medium")
dev.off()

cor_coph <- cor_cophenetic(high_svd,med_svd)
set.seed(2248)
R <- 10000
cor_coph_results <- numeric(R)
dend_mixed <- med_svd #shuffle dataset2_dend
for (i in 1:R){
  dend_mixed <- sample.dendrogram(dend_mixed, replace = F) #just swapping labels for a permutation p-value
  cor_coph_results[i] <- cor_cophenetic(high_svd, dend_mixed) # compare to 
}

mean(abs(cor_coph_results)>=cor_coph)
sum(abs(cor_coph_results)>=cor_coph)
cor_coph_results <- cor_coph_results %>% as.data.frame()

cor_coph_round <- cor_coph %>% round(., digits=3)
print(cor_coph_round)
```


# Comparing MS1 vs High
```{r}
base::load(here::here("data", "Dataset1", "dataset1-ms1_dedro.RData"))          
dataset1_clust <- ms1_drugclust_svd
dataset1_clustboot <- ms1_drugclustboot_svd
dataset1_distance <- ms1_moduleTraitCor_svd



clust_cols_D1 <- c(pink, blackcof, teal,yellow, blue)

dataset1_dend <- dataset1_clust %>% as.dendrogram() %>% color_branches(k=5, col=clust_cols_D1) %>%
    dendextend::set("branches_lwd", 3) 

set.seed(123)
compare_D1High<- dendlist(dataset1_dend , high_svd ) %>% untangle(method = "step2side")
compare_D1High %>% entanglement() #0.044

cor_cophenetic(dataset1_dend , high_svd)


pdf("../../4.mSystems/2.revisionv2/figs/tanglegram-D1high.pdf", width=6, height=5)
tanglegram(compare_D1High, sort=F, common_subtrees_color_lines=F,highlight_branches_lwd = F, 
           highlight_distinct_edges=F, main="", main_left="Dataset 1", main_right="High")
dev.off()

cor_coph <- cor_cophenetic(dataset1_dend, high_svd)
set.seed(2248)
R <- 10000
cor_coph_results <- numeric(R)
dend_mixed <- high_svd #shuffle dataset2_dend
for (i in 1:R){
  dend_mixed <- sample.dendrogram(dend_mixed, replace = F) #just swapping labels for a permutation p-value
  cor_coph_results[i] <- cor_cophenetic(dataset1_dend,dend_mixed) # compare to 
}

mean(abs(cor_coph_results)>=cor_coph)
sum(abs(cor_coph_results)>=cor_coph)
cor_coph_results <- cor_coph_results %>% as.data.frame()

cor_coph_round <- cor_coph %>% round(., digits=3)
print(cor_coph_round)
```


# Comparing MS1 vs Med
```{r}
set.seed(123)
compare_D1Med<- dendlist(dataset1_dend , med_svd ) %>% untangle(method = "step2side")
compare_D1Med %>% entanglement() #0.044

cor_cophenetic(dataset1_dend , med_svd)
#pdf("../../4.mSystems/2.revisionv2/figs/tanglegram-D1med.pdf", width=6, height=5)
tanglegram(compare_D1Med, sort=F, common_subtrees_color_lines=F,highlight_branches_lwd = F, 
           highlight_distinct_edges=F, main="", main_left="Dataset 1", main_right="High+Med")
#dev.off()

cor_coph <- cor_cophenetic(dataset1_dend, med_svd)
set.seed(2248)
R <- 10000
cor_coph_results <- numeric(R)
dend_mixed <- med_svd #shuffle dataset2_dend
for (i in 1:R){
  dend_mixed <- sample.dendrogram(dend_mixed, replace = F) #just swapping labels for a permutation p-value
  cor_coph_results[i] <- cor_cophenetic(dataset1_dend,dend_mixed) # compare to 
}

mean(abs(cor_coph_results)>=cor_coph)
sum(abs(cor_coph_results)>=cor_coph)


```